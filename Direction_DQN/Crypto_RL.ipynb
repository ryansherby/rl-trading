{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "from Direction_DQN.model import train_model, evaluate_model, Agent\n",
    "from Direction_DQN.utils import get_data, show_train_result, show_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = None\n",
    "\n",
    "hyperparams = {\n",
    "    'gamma': 0.99,  # discount factor\n",
    "    'epsilon': 1.0,  # exploration rate\n",
    "    'epsilon_min': 0.01,  # minimum exploration rate\n",
    "    'epsilon_decay': 0.95,  # decay rate for exploration prob\n",
    "    'learning_rate': 0.001,  # learning rate\n",
    "    'batch_size': 256,  # size of minibatch\n",
    "    'alpha': 0.6,  # alpha for prioritized experience replay\n",
    "    'beta_start': 0.4,  # initial value of beta\n",
    "    'beta_decay': 0.98, # decay rate for beta\n",
    "    \"beta_max\": 1, # minimum value of beta\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryansherby/Library/CloudStorage/OneDrive-Personal/Documents/Columbia/Reinforcement Learning/Project/Direction_DQN/alphas101.py:289: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inner[self.returns < 0] = stddev(self.returns, 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Excluding timestamp-like column alpha054 in Direction_DQN/data/btc.csv\n",
      "Warning: Excluding timestamp-like column alpha083 in Direction_DQN/data/btc.csv\n"
     ]
    }
   ],
   "source": [
    "btc = get_data('Direction_DQN/data/btc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = btc.drop(['Start', 'End','Open', 'High', 'Low', 'Volume', 'Market Cap','Average','VWAP','Close','Returns', 'Log Returns'],axis=1).columns\n",
    "\n",
    "price_col = btc['Log Returns'].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = btc[btc['Start'] < '2024-01-01']\n",
    "test_data = btc[btc['Start'] >= '2024-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data[feature_cols].to_numpy()\n",
    "train_y = train_data[price_col].to_numpy().reshape(-1,1)\n",
    "\n",
    "train_X = rs.fit_transform(train_X)\n",
    "\n",
    "test_X = test_data[feature_cols].to_numpy()\n",
    "test_y = test_data[price_col].to_numpy().reshape(-1,1)\n",
    "\n",
    "test_X = rs.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4916, 83)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100 - Train Position: 0.4277                                            Train Loss: 0.1480\n",
      "Episode 2/100 - Train Position: 10.4063                                            Train Loss: 0.1570\n",
      "Episode 3/100 - Train Position: 13.8781                                            Train Loss: 0.0415\n",
      "Episode 4/100 - Train Position: 12.4649                                            Train Loss: 0.0253\n",
      "Episode 5/100 - Train Position: 3.4443                                            Train Loss: 0.0266\n",
      "Episode 6/100 - Train Position: 9.3360                                            Train Loss: 6.8193\n",
      "Episode 7/100 - Train Position: -9.4136                                            Train Loss: 36.2020\n",
      "Episode 8/100 - Train Position: 10.6516                                            Train Loss: 0.0087\n",
      "Episode 9/100 - Train Position: -0.8150                                            Train Loss: 0.0142\n",
      "Episode 10/100 - Train Position: 4.7501                                            Train Loss: 0.0154\n",
      "Episode 11/100 - Train Position: -8.1181                                            Train Loss: 0.0126\n",
      "Episode 12/100 - Train Position: -5.9426                                            Train Loss: 0.0128\n",
      "Episode 13/100 - Train Position: -2.4671                                            Train Loss: 0.0142\n",
      "Episode 14/100 - Train Position: -11.5050                                            Train Loss: 0.0143\n",
      "Episode 15/100 - Train Position: -8.9535                                            Train Loss: 0.0156\n",
      "Episode 16/100 - Train Position: -13.9925                                            Train Loss: 0.0215\n",
      "Episode 17/100 - Train Position: -12.9394                                            Train Loss: 0.0178\n",
      "Episode 18/100 - Train Position: -15.7005                                            Train Loss: 0.0189\n",
      "Episode 19/100 - Train Position: -12.8255                                            Train Loss: 0.0204\n",
      "Episode 20/100 - Train Position: -13.5085                                            Train Loss: 0.0214\n",
      "Episode 21/100 - Train Position: -14.2479                                            Train Loss: 0.0227\n",
      "Episode 22/100 - Train Position: -13.8372                                            Train Loss: 0.0239\n",
      "Episode 23/100 - Train Position: -13.2130                                            Train Loss: 0.0243\n",
      "Episode 24/100 - Train Position: -15.6493                                            Train Loss: 0.0255\n",
      "Episode 25/100 - Train Position: -11.0407                                            Train Loss: 0.0257\n",
      "Episode 26/100 - Train Position: -13.0463                                            Train Loss: 0.0266\n",
      "Episode 27/100 - Train Position: -14.6492                                            Train Loss: 0.0273\n",
      "Episode 28/100 - Train Position: -0.7756                                            Train Loss: 0.0277\n",
      "Episode 29/100 - Train Position: -4.0959                                            Train Loss: 0.0291\n",
      "Episode 30/100 - Train Position: 4.1710                                            Train Loss: 0.0302\n",
      "Episode 31/100 - Train Position: -7.4299                                            Train Loss: 0.0309\n",
      "Episode 32/100 - Train Position: -3.3905                                            Train Loss: 0.0315\n",
      "Episode 33/100 - Train Position: -8.5622                                            Train Loss: 0.0322\n",
      "Episode 34/100 - Train Position: -14.5782                                            Train Loss: 0.0333\n",
      "Episode 35/100 - Train Position: -4.0762                                            Train Loss: 0.0342\n",
      "Episode 36/100 - Train Position: -14.2394                                            Train Loss: 0.0341\n",
      "Episode 37/100 - Train Position: -15.3220                                            Train Loss: 0.0353\n",
      "Episode 38/100 - Train Position: -15.7976                                            Train Loss: 0.0340\n",
      "Episode 39/100 - Train Position: -12.6817                                            Train Loss: 0.0371\n",
      "Episode 40/100 - Train Position: -12.8286                                            Train Loss: 0.0372\n",
      "Episode 41/100 - Train Position: -13.5546                                            Train Loss: 0.0371\n",
      "Episode 42/100 - Train Position: -13.1453                                            Train Loss: 0.0384\n",
      "Episode 43/100 - Train Position: -13.5147                                            Train Loss: 0.0400\n",
      "Episode 44/100 - Train Position: -14.0968                                            Train Loss: 0.0387\n",
      "Episode 45/100 - Train Position: -12.4496                                            Train Loss: 0.0376\n",
      "Episode 46/100 - Train Position: -13.6352                                            Train Loss: 0.0396\n",
      "Episode 47/100 - Train Position: -12.8904                                            Train Loss: 0.0405\n",
      "Episode 48/100 - Train Position: -12.7289                                            Train Loss: 0.0407\n",
      "Episode 49/100 - Train Position: -13.7761                                            Train Loss: 0.0407\n",
      "Episode 50/100 - Train Position: -13.5496                                            Train Loss: 0.0414\n",
      "Episode 51/100 - Train Position: -13.5955                                            Train Loss: 0.0425\n",
      "Episode 52/100 - Train Position: -13.3014                                            Train Loss: 0.0421\n",
      "Episode 53/100 - Train Position: -13.9877                                            Train Loss: 0.0448\n",
      "Episode 54/100 - Train Position: -14.1811                                            Train Loss: 0.0436\n",
      "Episode 55/100 - Train Position: -12.1871                                            Train Loss: 0.0434\n",
      "Episode 56/100 - Train Position: -13.6199                                            Train Loss: 0.0444\n",
      "Episode 57/100 - Train Position: -13.5391                                            Train Loss: 0.0442\n",
      "Episode 58/100 - Train Position: -14.1024                                            Train Loss: 0.0454\n",
      "Episode 59/100 - Train Position: -13.6284                                            Train Loss: 0.0462\n",
      "Episode 60/100 - Train Position: -12.1692                                            Train Loss: 0.0461\n",
      "Episode 61/100 - Train Position: -13.1637                                            Train Loss: 0.0468\n",
      "Episode 62/100 - Train Position: -13.2117                                            Train Loss: 0.0478\n",
      "Episode 63/100 - Train Position: -14.3141                                            Train Loss: 0.0479\n",
      "Episode 64/100 - Train Position: -13.4087                                            Train Loss: 0.0508\n",
      "Episode 65/100 - Train Position: -13.8177                                            Train Loss: 0.0506\n",
      "Episode 66/100 - Train Position: -13.3130                                            Train Loss: 0.0499\n",
      "Episode 67/100 - Train Position: -14.7157                                            Train Loss: 0.0519\n",
      "Episode 68/100 - Train Position: -13.4070                                            Train Loss: 0.0540\n",
      "Episode 69/100 - Train Position: -13.5763                                            Train Loss: 0.0524\n",
      "Episode 70/100 - Train Position: -13.8134                                            Train Loss: 0.0529\n",
      "Episode 71/100 - Train Position: -13.6911                                            Train Loss: 0.0542\n",
      "Episode 72/100 - Train Position: -12.8835                                            Train Loss: 0.0558\n",
      "Episode 73/100 - Train Position: -14.2812                                            Train Loss: 0.0571\n",
      "Episode 74/100 - Train Position: -12.9447                                            Train Loss: 0.0596\n",
      "Episode 75/100 - Train Position: -13.3126                                            Train Loss: 0.0603\n",
      "Episode 76/100 - Train Position: -12.5159                                            Train Loss: 0.0645\n",
      "Episode 77/100 - Train Position: -13.2526                                            Train Loss: 0.0646\n",
      "Episode 78/100 - Train Position: -13.3989                                            Train Loss: 0.0654\n",
      "Episode 79/100 - Train Position: -13.2828                                            Train Loss: 0.0677\n",
      "Episode 80/100 - Train Position: -13.8504                                            Train Loss: 0.0723\n",
      "Episode 81/100 - Train Position: -13.5908                                            Train Loss: 0.0713\n",
      "Episode 82/100 - Train Position: -14.1954                                            Train Loss: 0.0713\n",
      "Episode 83/100 - Train Position: -13.8614                                            Train Loss: 0.0713\n",
      "Episode 84/100 - Train Position: -14.1761                                            Train Loss: 0.0728\n",
      "Episode 85/100 - Train Position: -13.4899                                            Train Loss: 0.0720\n",
      "Episode 86/100 - Train Position: -13.0059                                            Train Loss: 0.0746\n",
      "Episode 87/100 - Train Position: -13.3005                                            Train Loss: 0.0762\n",
      "Episode 88/100 - Train Position: -13.2547                                            Train Loss: 0.0796\n",
      "Episode 89/100 - Train Position: -13.2513                                            Train Loss: 0.0773\n",
      "Episode 90/100 - Train Position: -13.0217                                            Train Loss: 0.0782\n",
      "Episode 91/100 - Train Position: -12.5849                                            Train Loss: 0.0769\n",
      "Episode 92/100 - Train Position: -14.1573                                            Train Loss: 0.0818\n",
      "Episode 93/100 - Train Position: -13.9777                                            Train Loss: 0.0808\n",
      "Episode 94/100 - Train Position: -13.1928                                            Train Loss: 0.0805\n",
      "Episode 95/100 - Train Position: -13.0364                                            Train Loss: 0.0802\n",
      "Episode 96/100 - Train Position: -13.3380                                            Train Loss: 0.0802\n",
      "Episode 97/100 - Train Position: -14.2228                                            Train Loss: 0.0787\n",
      "Episode 98/100 - Train Position: -14.0474                                            Train Loss: 0.0778\n",
      "Episode 99/100 - Train Position: -13.4038                                            Train Loss: 0.0785\n",
      "Episode 100/100 - Train Position: -13.7571                                            Train Loss: 0.0817\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=len(feature_cols),\n",
    "    action_size=2,\n",
    "    strategy='double-dqn',\n",
    "    hyperparams=hyperparams,\n",
    "    device = 'mps',\n",
    "    memory_size=1000,\n",
    "    pretrained=False,\n",
    "    model_name='directional'\n",
    ")\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    ep, total_eps, reward, average_loss  = train_model(agent, episode, train_X, train_y, ep_count=episodes,batch_size=hyperparams['batch_size']\n",
    "                                            )\n",
    "\n",
    "    # Show training results\n",
    "    show_train_result(ep, total_eps, reward, average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Rewards: -0.7526\n",
      "Action Counts: Counter({0: 365})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for episode in range(1,2):\n",
    "    \n",
    "    reward, history  = evaluate_model(agent, episode, test_X, test_y)\n",
    "                                            \n",
    "    # Show test results\n",
    "    show_test_result(reward, Counter([h['pred'] for h in history]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_drop = np.argmin(np.array([h['reward'] for h in history]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'true': 0, 'pred': 0, 'reward': -0.1167454794049263}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[worst_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
