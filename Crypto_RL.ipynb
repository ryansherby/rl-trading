{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "from Direction_DQN.model import train_model, evaluate_model, Agent\n",
    "from Direction_DQN.utils import get_data, show_train_result, show_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = None\n",
    "\n",
    "hyperparams = {\n",
    "    'gamma': 0.99,  # discount factor\n",
    "    'epsilon': 1.0,  # exploration rate\n",
    "    'epsilon_min': 0.01,  # minimum exploration rate\n",
    "    'epsilon_decay': 0.95,  # decay rate for exploration prob\n",
    "    'learning_rate': 0.001,  # learning rate\n",
    "    'batch_size': 128,  # size of minibatch\n",
    "    'alpha': 0.6,  # alpha for prioritized experience replay\n",
    "    'beta_start': 0.4,  # initial value of beta\n",
    "    'beta_decay': 0.98, # decay rate for beta\n",
    "    \"beta_max\": 1, # minimum value of beta\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryansherby/Library/CloudStorage/OneDrive-Personal/Documents/Columbia/Reinforcement Learning/Project/Direction_DQN/alphas101.py:289: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inner[self.returns < 0] = stddev(self.returns, 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Excluding timestamp-like column alpha054 in Direction_DQN/data/btc.csv\n",
      "Warning: Excluding timestamp-like column alpha083 in Direction_DQN/data/btc.csv\n"
     ]
    }
   ],
   "source": [
    "btc = get_data('Direction_DQN/data/btc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = btc.drop(['Start', 'End','Open', 'High', 'Low', 'Volume', 'Market Cap','Average','VWAP','Close','Returns', 'Log Returns'],axis=1).columns\n",
    "\n",
    "price_col = btc['Log Returns'].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = btc[btc['Start'] < '2024-01-01']\n",
    "test_data = btc[btc['Start'] >= '2024-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data[feature_cols].to_numpy()\n",
    "train_y = train_data[price_col].to_numpy().reshape(-1,1)\n",
    "\n",
    "train_X = rs.fit_transform(train_X)\n",
    "\n",
    "test_X = test_data[feature_cols].to_numpy()\n",
    "test_y = test_data[price_col].to_numpy().reshape(-1,1)\n",
    "\n",
    "test_X = rs.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4916, 83)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100 - Train Position: 0.3590                                            Train Loss: 0.3344\n",
      "Episode 2/100 - Train Position: 18.3315                                            Train Loss: 0.3074\n",
      "Episode 3/100 - Train Position: 20.5033                                            Train Loss: 4.8460\n",
      "Episode 4/100 - Train Position: 21.3146                                            Train Loss: 0.0268\n",
      "Episode 5/100 - Train Position: 17.4088                                            Train Loss: 0.0072\n",
      "Episode 6/100 - Train Position: 22.2741                                            Train Loss: 60.6156\n",
      "Episode 7/100 - Train Position: 12.8941                                            Train Loss: 307.5991\n",
      "Episode 8/100 - Train Position: 13.9391                                            Train Loss: 36.6598\n",
      "Episode 9/100 - Train Position: 12.7612                                            Train Loss: 17.8599\n",
      "Episode 10/100 - Train Position: 2.3731                                            Train Loss: 5.5871\n",
      "Episode 11/100 - Train Position: -12.8960                                            Train Loss: 3.4970\n",
      "Episode 12/100 - Train Position: -3.6789                                            Train Loss: 1.7275\n",
      "Episode 13/100 - Train Position: 9.1670                                            Train Loss: 0.7825\n",
      "Episode 14/100 - Train Position: 3.3867                                            Train Loss: 0.3505\n",
      "Episode 15/100 - Train Position: 14.2643                                            Train Loss: 0.1088\n",
      "Episode 16/100 - Train Position: 9.6284                                            Train Loss: 0.0205\n",
      "Episode 17/100 - Train Position: 3.3246                                            Train Loss: 0.0203\n",
      "Episode 18/100 - Train Position: 6.7319                                            Train Loss: 0.0199\n",
      "Episode 19/100 - Train Position: 1.2632                                            Train Loss: 0.0211\n",
      "Episode 20/100 - Train Position: -2.7374                                            Train Loss: 0.0217\n",
      "Episode 21/100 - Train Position: 2.8081                                            Train Loss: 0.0229\n",
      "Episode 22/100 - Train Position: 4.1385                                            Train Loss: 0.0238\n",
      "Episode 23/100 - Train Position: 6.8559                                            Train Loss: 0.0248\n",
      "Episode 24/100 - Train Position: -3.1211                                            Train Loss: 0.0260\n",
      "Episode 25/100 - Train Position: 4.1065                                            Train Loss: 0.0265\n",
      "Episode 26/100 - Train Position: 1.1218                                            Train Loss: 0.0272\n",
      "Episode 27/100 - Train Position: 2.6430                                            Train Loss: 0.0281\n",
      "Episode 28/100 - Train Position: 2.5275                                            Train Loss: 0.0283\n",
      "Episode 29/100 - Train Position: 12.8091                                            Train Loss: 0.0289\n",
      "Episode 30/100 - Train Position: 2.0051                                            Train Loss: 0.0294\n",
      "Episode 31/100 - Train Position: 7.2689                                            Train Loss: 0.0295\n",
      "Episode 32/100 - Train Position: 10.3429                                            Train Loss: 0.0301\n",
      "Episode 33/100 - Train Position: 11.6753                                            Train Loss: 0.0306\n",
      "Episode 34/100 - Train Position: -0.2460                                            Train Loss: 0.0316\n",
      "Episode 35/100 - Train Position: 1.0205                                            Train Loss: 0.0321\n",
      "Episode 36/100 - Train Position: -10.8596                                            Train Loss: 0.0324\n",
      "Episode 37/100 - Train Position: -9.2735                                            Train Loss: 0.0327\n",
      "Episode 38/100 - Train Position: -7.0450                                            Train Loss: 0.0326\n",
      "Episode 39/100 - Train Position: 0.4629                                            Train Loss: 0.0332\n",
      "Episode 40/100 - Train Position: 3.4491                                            Train Loss: 0.0339\n",
      "Episode 41/100 - Train Position: -4.6387                                            Train Loss: 0.0346\n",
      "Episode 42/100 - Train Position: -8.9484                                            Train Loss: 0.0347\n",
      "Episode 43/100 - Train Position: -6.9504                                            Train Loss: 0.0355\n",
      "Episode 44/100 - Train Position: 3.0183                                            Train Loss: 0.0357\n",
      "Episode 45/100 - Train Position: -11.9218                                            Train Loss: 0.0366\n",
      "Episode 46/100 - Train Position: 2.6753                                            Train Loss: 0.0369\n",
      "Episode 47/100 - Train Position: -10.4070                                            Train Loss: 0.0372\n",
      "Episode 48/100 - Train Position: -6.9506                                            Train Loss: 0.0373\n",
      "Episode 49/100 - Train Position: -10.5244                                            Train Loss: 0.0384\n",
      "Episode 50/100 - Train Position: -13.4800                                            Train Loss: 0.0394\n",
      "Episode 51/100 - Train Position: -10.9143                                            Train Loss: 0.0397\n",
      "Episode 52/100 - Train Position: -12.6340                                            Train Loss: 0.0403\n",
      "Episode 53/100 - Train Position: -14.1273                                            Train Loss: 0.0412\n",
      "Episode 54/100 - Train Position: -13.2143                                            Train Loss: 0.0421\n",
      "Episode 55/100 - Train Position: -13.1769                                            Train Loss: 0.0421\n",
      "Episode 56/100 - Train Position: -14.6214                                            Train Loss: 0.0423\n",
      "Episode 57/100 - Train Position: -13.5994                                            Train Loss: 0.0435\n",
      "Episode 58/100 - Train Position: -12.5445                                            Train Loss: 0.0451\n",
      "Episode 59/100 - Train Position: -13.9098                                            Train Loss: 0.0452\n",
      "Episode 60/100 - Train Position: -13.0578                                            Train Loss: 0.0457\n",
      "Episode 61/100 - Train Position: -13.5633                                            Train Loss: 0.0472\n",
      "Episode 62/100 - Train Position: -13.5966                                            Train Loss: 0.0467\n",
      "Episode 63/100 - Train Position: -13.1368                                            Train Loss: 0.0477\n",
      "Episode 64/100 - Train Position: -13.4683                                            Train Loss: 0.0479\n",
      "Episode 65/100 - Train Position: -13.2823                                            Train Loss: 0.0478\n",
      "Episode 66/100 - Train Position: -13.5891                                            Train Loss: 0.0489\n",
      "Episode 67/100 - Train Position: -9.4265                                            Train Loss: 0.0489\n",
      "Episode 68/100 - Train Position: -13.7492                                            Train Loss: 0.0499\n",
      "Episode 69/100 - Train Position: -12.8354                                            Train Loss: 0.0512\n",
      "Episode 70/100 - Train Position: -13.4649                                            Train Loss: 0.0525\n",
      "Episode 71/100 - Train Position: -14.1096                                            Train Loss: 0.0526\n",
      "Episode 72/100 - Train Position: -12.6534                                            Train Loss: 0.0547\n",
      "Episode 73/100 - Train Position: -13.5896                                            Train Loss: 0.0562\n",
      "Episode 74/100 - Train Position: -13.7795                                            Train Loss: 0.0586\n",
      "Episode 75/100 - Train Position: -13.4327                                            Train Loss: 0.0592\n",
      "Episode 76/100 - Train Position: -13.6794                                            Train Loss: 0.0607\n",
      "Episode 77/100 - Train Position: -12.6115                                            Train Loss: 0.0624\n",
      "Episode 78/100 - Train Position: -13.0379                                            Train Loss: 0.0629\n",
      "Episode 79/100 - Train Position: -7.7112                                            Train Loss: 0.0632\n",
      "Episode 80/100 - Train Position: -13.2413                                            Train Loss: 0.0641\n",
      "Episode 81/100 - Train Position: -14.0029                                            Train Loss: 0.0639\n",
      "Episode 82/100 - Train Position: -13.8719                                            Train Loss: 0.0645\n",
      "Episode 83/100 - Train Position: -13.6834                                            Train Loss: 0.0630\n",
      "Episode 84/100 - Train Position: -13.7988                                            Train Loss: 0.0631\n",
      "Episode 85/100 - Train Position: -13.5089                                            Train Loss: 0.0638\n",
      "Episode 86/100 - Train Position: -16.9585                                            Train Loss: 0.0629\n",
      "Episode 87/100 - Train Position: -13.0034                                            Train Loss: 0.0621\n",
      "Episode 88/100 - Train Position: -13.8184                                            Train Loss: 0.0639\n",
      "Episode 89/100 - Train Position: -13.5237                                            Train Loss: 0.0632\n",
      "Episode 90/100 - Train Position: -13.0638                                            Train Loss: 0.0648\n",
      "Episode 91/100 - Train Position: -13.1383                                            Train Loss: 0.0652\n",
      "Episode 92/100 - Train Position: -14.6321                                            Train Loss: 0.0649\n",
      "Episode 93/100 - Train Position: -13.1922                                            Train Loss: 0.0662\n",
      "Episode 94/100 - Train Position: -12.6892                                            Train Loss: 0.0650\n",
      "Episode 95/100 - Train Position: -13.4232                                            Train Loss: 0.0633\n",
      "Episode 96/100 - Train Position: -13.9660                                            Train Loss: 0.0623\n",
      "Episode 97/100 - Train Position: -12.7647                                            Train Loss: 0.0604\n",
      "Episode 98/100 - Train Position: -13.0816                                            Train Loss: 0.0612\n",
      "Episode 99/100 - Train Position: -13.2044                                            Train Loss: 0.0609\n",
      "Episode 100/100 - Train Position: -13.5365                                            Train Loss: 0.0601\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=len(feature_cols),\n",
    "    action_size=2,\n",
    "    strategy='double-dqn',\n",
    "    hyperparams=hyperparams,\n",
    "    device = 'mps',\n",
    "    memory_size=1000,\n",
    "    pretrained=False,\n",
    "    model_name='directional'\n",
    ")\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    ep, total_eps, reward, average_loss  = train_model(agent, episode, train_X, train_y, ep_count=episodes,batch_size=hyperparams['batch_size']\n",
    "                                            )\n",
    "\n",
    "    # Show training results\n",
    "    show_train_result(ep, total_eps, reward, average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Rewards: -0.7526\n",
      "Action Counts: Counter({0: 365})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for episode in range(1,2):\n",
    "    \n",
    "    reward, history  = evaluate_model(agent, episode, test_X, test_y)\n",
    "                                            \n",
    "    # Show test results\n",
    "    show_test_result(reward, Counter([h['pred'] for h in history]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_drop = np.argmin(np.array([h['reward'] for h in history]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'true': 0, 'pred': 0, 'reward': -0.1167454794049263}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[worst_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
